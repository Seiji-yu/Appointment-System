import { R as ResolvedAxiomConfig } from './config-DT-xvV7w.cjs';
import { SerializedError } from 'vitest';
import { Reporter, TestSuite, TestCase, TestModule, TestRunEndReason } from 'vitest/node.js';
import { ZodObject, ZodDefault, z } from 'zod';
import { $ZodObject } from 'zod/v4/core';
import '@opentelemetry/api';

/**
 * Creates a scorer that is both ergonomic for authors and fully generic for
 * the type-system.
 *
 * • If the callback returns a `number`, it is wrapped into { name, score }
 * • If it returns a full `Score`, we only ensure the `name` field is present
 */
declare function createScorer<TInput = unknown, TExpected = unknown, TOutput = unknown>(name: string, fn: (args: {
    input: TInput;
    expected: TExpected;
    output: TOutput;
}) => number | Score | Promise<number | Score>): Scorer<TInput, TExpected, TOutput>;

type Score = {
    name: string;
    score: number | null;
    metadata?: Record<string, any>;
};

type Scorer<TInput, TExpected, TOutput> = (args: {
    input: TInput;
    expected: TExpected;
    output: TOutput;
}) => Score | Promise<Score>;

/** Extract the input type from CollectionRecord[] */
type InputOf<Data extends readonly CollectionRecord<any, any>[]> = Data[number] extends CollectionRecord<infer I, any> ? I : never;
/** Extract the expected type from CollectionRecord[] */
type ExpectedOf<Data extends readonly CollectionRecord<any, any>[]> = Data[number] extends CollectionRecord<any, infer E> ? E : never;
/**
 * Function type for evaluation tasks that process input data and produce output.
 *
 * Used with {@link EvalParams} to define the task that will be evaluated against a dataset.
 * The task output will be scored by functions defined in {@link EvalParams.scorers}.
 *
 * @experimental This API is experimental and may change in future versions.
 *
 * @param input - The input data to process
 * @param expected - The expected output for comparison/validation
 * @returns The task output, Promise, or AsyncIterable for streaming
 *
 * @example
 * ```typescript
 * const textGenerationTask: EvalTask<string, string, string> = async ({ input, expected }) => {
 *   const result = await generateText({
 *     model: myModel,
 *     prompt: input
 *   });
 *   return result.text;
 * };
 * ```
 */
type EvalTask<TInput extends string | Record<string, any>, TExpected extends string | Record<string, any>, TOutput extends string | Record<string, any>> = (args: {
    input: TInput;
    expected: TExpected;
}) => TOutput | Promise<TOutput> | AsyncIterable<TOutput>;
/**
 * Record type representing a single data point in an evaluation dataset.
 *
 * @experimental This API is experimental and may change in future versions.
 */
type CollectionRecord<TInput extends string | Record<string, any>, TExpected extends string | Record<string, any>> = {
    /** The input data for the evaluation case */
    input: TInput;
    /** The expected output for comparison/validation */
    expected: TExpected;
    /** Optional metadata for the record */
    metadata?: Record<string, unknown>;
};
/**
 * Configuration parameters for running an evaluation.
 *
 * Used with {@link Eval} to define how an evaluation should be executed.
 * Results are captured in {@link EvalReport} format.
 *
 * @experimental This API is experimental and may change in future versions.
 */
type EvalParams<TInput extends string | Record<string, any>, TExpected extends string | Record<string, any>, TOutput extends string | Record<string, any>> = {
    /** Function that returns the dataset with input/expected pairs for evaluation */
    data: () => readonly CollectionRecord<TInput, TExpected>[] | Promise<readonly CollectionRecord<TInput, TExpected>[]>;
    /** The task function to evaluate */
    task: EvalTask<TInput, TExpected, TOutput>;
    /** Array of scoring functions to evaluate the task output */
    scorers: ReadonlyArray<Scorer<TInput, TExpected, TOutput>>;
    /** Optional metadata for the evaluation */
    metadata?: Record<string, unknown>;
    /** Optional timeout in milliseconds for task execution */
    timeout?: number;
    /** Optional reduction of flag namespace */
    configFlags?: string[];
};
type RuntimeFlagLog = {
    kind: 'introduced';
    value: unknown;
} | {
    kind: 'replaced';
    value: unknown;
    default: unknown;
};
type RuntimeFlagMap = Record<string, RuntimeFlagLog>;
type Evaluation = {
    id: string;
    name: string;
    type: string;
    version: string;
    baseline: {
        id: string | undefined;
        name: string | undefined;
    };
    collection: {
        name: string;
        size: number;
    };
    prompt: {
        model: string;
        params: Record<string, unknown>;
    };
    duration: number;
    status: string;
    traceId: string;
    runAt: string;
    tags: string[];
    user: {
        name: string | undefined;
        email: string | undefined;
    };
    cases: Case[];
    flagConfig?: Record<string, any>;
};
type Case = {
    index: number;
    input: string;
    output: string;
    expected: string;
    duration: string;
    status: string;
    scores: Record<string, {
        name: string;
        value: number;
        metadata: Record<string, any>;
    }>;
    runAt: string;
    spanId: string;
    traceId: string;
    task?: Task;
    runtimeFlags?: RuntimeFlagMap;
};
type Chat = {
    operation: string;
    capability: string;
    step: string;
    request: {
        max_token: string;
        model: string;
        temperature: number;
    };
    response: {
        finish_reasons: string;
    };
    usage: {
        input_tokens: number;
        output_tokens: number;
    };
};
type Task = {
    name: string;
    output: string;
    trial: number;
    type: string;
    error?: string;
    chat: Chat;
};
/**
 * Complete report for a single evaluation case including results and metadata.
 *
 * Generated for each test case when running {@link Eval} with {@link EvalParams}.
 * Contains all {@link Score} results and execution metadata.
 *
 * @experimental This API is experimental and may change in future versions.
 */
type EvalCaseReport = {
    /** Order/index of this case in the evaluation suite */
    index: number;
    /** Name of the evaluation */
    name: string;
    /** Input data that was provided to the {@link EvalTask} */
    input: string | Record<string, any>;
    /** Output produced by the {@link EvalTask} */
    output: string | Record<string, any>;
    /** Expected output for comparison */
    expected: string | Record<string, any>;
    /** Array of {@link Score} results from all scorers that were run */
    scores: Record<string, Score>;
    /** Any errors that occurred during evaluation */
    errors: Error[] | null;
    /** Status of the evaluation case */
    status: 'success' | 'fail' | 'pending';
    /** Duration in milliseconds for the entire case */
    duration: number | undefined;
    /** Timestamp when the case started */
    startedAt: number | undefined;
    /** Flags accessed outside of the picked flags scope for this case */
    outOfScopeFlags?: {
        flagPath: string;
        accessedAt: number;
        stackTrace: string[];
    }[];
    /** Flags that are in scope for this evaluation */
    pickedFlags?: string[];
    /** Runtime flags actually used during this case */
    runtimeFlags?: RuntimeFlagMap;
};
type OutOfScopeFlag = {
    flagPath: string;
    count: number;
    firstAccessedAt: number;
    lastAccessedAt: number;
    stackTrace: string[];
};
type EvaluationReport = {
    id: string;
    name: string;
    version: string;
    baseline: Evaluation | undefined;
    /** Flags that are in scope for this evaluation */
    configFlags?: string[];
    /** Full flag configuration for this evaluation run */
    flagConfig?: Record<string, any>;
    /** Summary of all flags accessed outside of picked flags scope across all cases */
    outOfScopeFlags?: OutOfScopeFlag[];
    /** End-of-suite config snapshot for console printing only */
    configEnd?: {
        flags?: Record<string, any>;
        pickedFlags?: string[];
        overrides?: Record<string, any>;
    };
};

declare module 'vitest' {
    interface TestSuiteMeta {
        evaluation: EvaluationReport;
    }
    interface TaskMeta {
        case: EvalCaseReport;
        evaluation: EvaluationReport;
    }
    interface ProvidedContext {
        baseline?: string;
        debug?: boolean;
        overrides?: Record<string, any>;
        axiomConfig?: ResolvedAxiomConfig;
    }
}
/**
 * Creates and registers an evaluation suite with the given name and parameters.
 *
 * This function sets up a complete evaluation pipeline that will run your {@link EvalTask}
 * against a dataset, score the results, and provide detailed {@link EvalCaseReport} reporting.
 *
 * @experimental This API is experimental and may change in future versions.
 *
 * @param name - Human-readable name for the evaluation suite
 * @param params - {@link EvalParams} configuration parameters for the evaluation
 *
 * @example
 * ```typescript
 * import { experimental_Eval as Eval } from 'axiom/ai/evals';
 *
 * Eval('Text Generation Quality', {
 *   data: async () => [
 *     { input: 'Explain photosynthesis', expected: 'Plants convert light to energy...' },
 *     { input: 'What is gravity?', expected: 'Gravity is a fundamental force...' }
 *   ],
 *   task: async ({ input }) => {
 *     const result = await generateText({
 *       model: yourModel,
 *       prompt: input
 *     });
 *     return result.text;
 *   },
 *   scorers: [similarityScorer, factualAccuracyScorer],
 * });
 * ```
 */
declare function Eval<const Data extends readonly {
    input: any;
    expected: any;
}[], Out extends string | Record<string, any>, TaskFn extends EvalTask<InputOf<Data>, ExpectedOf<Data>, Out>, In = InputOf<Data>, Exp = ExpectedOf<Data>>(name: string, params: {
    data: () => Data | Promise<Data>;
    task: TaskFn;
    scorers: ReadonlyArray<Scorer<In, Exp, Out>>;
    metadata?: Record<string, unknown>;
    timeout?: number;
    configFlags?: string[];
}): void;
/**
 *
 */
declare function Eval<TInput extends string | Record<string, any>, TExpected extends string | Record<string, any>, TOutput extends string | Record<string, any>>(name: string, params: EvalParams<TInput, TExpected, TOutput>): void;

/**
 * Custom Vitest reporter for Axiom AI evaluations.
 *
 * This reporter collects evaluation results and scores from tests
 * and processes them for further analysis and reporting.
 *
 */
declare class AxiomReporter implements Reporter {
    startTime: number;
    start: number;
    private _endOfRunConfigEnd;
    private _suiteData;
    private _baselines;
    onTestRunStart(): void;
    onTestSuiteReady(_testSuite: TestSuite): Promise<void>;
    onTestCaseReady(test: TestCase): void;
    onTestSuiteResult(testSuite: TestSuite): Promise<void>;
    onTestRunEnd(_testModules: ReadonlyArray<TestModule>, _errors: ReadonlyArray<SerializedError>, _reason: TestRunEndReason): Promise<void>;
    private printCaseResult;
    /**
     * Calculate average scores per scorer for a suite
     */
    private calculateScorerAverages;
    /**
     * Calculate average score for a specific scorer from baseline data
     */
    private calculateBaselineScorerAverage;
    /**
     * Calculate flag diff between current run and baseline (filtered by configFlags)
     */
    private calculateFlagDiff;
    /**
     * End-of-suite config summary (console only)
     */
    private printConfigEnd;
}

type DefaultMaxDepth = 8;
type HasDefaults<S> = S extends {
    _zod: {
        def: {
            defaultValue: unknown;
        };
    };
} ? true : S extends $ZodObject<infer Shape> | ZodObject<infer Shape> ? {
    [K in keyof Shape]: HasDefaults<Shape[K]>;
} extends Record<keyof Shape, true> ? true : false : false;
type UnwrapSchema<T> = T extends ZodDefault<infer U> ? U : T;
type AllFieldsHaveDefaults<Schema> = Schema extends {
    _zod: {
        def: {
            defaultValue: unknown;
        };
    };
} ? true : HasDefaults<UnwrapSchema<Schema>>;
interface AppScopeConfig<FlagSchema extends ZodObject<any> | undefined = undefined, FactSchema extends ZodObject<any> | undefined = undefined> {
    flagSchema: FlagSchema;
    factSchema?: FactSchema;
}
/**
 * Recursive type to extract all possible paths from an object type.
 * Uses stack-based depth limiting for better performance.
 *
 * @template T - The object type to extract paths from
 * @template Stack - Internal stack counter (do not set manually)
 * @template MaxDepth - Maximum recursion depth (default: 8 for good balance)
 */
type ObjectPaths<T, Stack extends unknown[] = [], MaxDepth extends number = DefaultMaxDepth> = Stack['length'] extends MaxDepth ? never : T extends object ? {
    [K in keyof T]-?: K extends string | number ? `${K}` | `${K}.${ObjectPaths<T[K], [1, ...Stack], MaxDepth>}` : never;
}[keyof T] : never;
type ObjectPathValue<T, P extends string> = P extends keyof T ? T[P] : P extends `${infer K}.${infer Rest}` ? K extends keyof T ? ObjectPathValue<T[K], Rest> : never : never;
/**
 * Generate deep nested paths from flag schema.
 *
 * @template T - ZodObject to extract paths from
 * @template MaxDepth - Maximum recursion depth (default: 8, override for deeper nesting)
 * @example
 * // Default 8-level depth
 * type Paths = DotPaths<MySchema>
 *
 * // Custom depth for deeper nesting (impacts performance)
 * type DeepPaths = DotPaths<MySchema, 12>
 */
type DotPaths<T extends ZodObject<any>, MaxDepth extends number = DefaultMaxDepth> = {
    [NS in keyof T['shape']]: (string & NS) | {
        [P in ObjectPaths<z.output<UnwrapSchema<T['shape'][NS]>>, [
        ], MaxDepth>]: `${string & NS}.${P}`;
    }[ObjectPaths<z.output<UnwrapSchema<T['shape'][NS]>>, [], MaxDepth>];
}[keyof T['shape']];
type PathValue<T extends ZodObject<any>, P extends string> = P extends `${infer NS}.${infer Rest}` ? NS extends keyof T['shape'] ? ObjectPathValue<z.output<UnwrapSchema<T['shape'][NS]>>, Rest> : never : P extends keyof T['shape'] ? z.output<UnwrapSchema<T['shape'][P]>> : never;
type DotNotationFlagFunction<FS extends ZodObject<any> | undefined> = FS extends ZodObject<any> ? <P extends DotPaths<FS>>(path: P) => PathValue<FS, P> : never;
type FactFunction<SC extends ZodObject<any> | undefined> = SC extends ZodObject<any> ? <P extends DotPaths<SC> & string>(name: P, value: PathValue<SC, P>) => void : never;
type OverrideFlagsFunction<FS extends ZodObject<any> | undefined> = FS extends ZodObject<any> ? (partial: {
    [K in DotPaths<FS>]?: PathValue<FS, K>;
}) => void : (partial: Record<string, any>) => void;
type WithFlagsFunction<FS extends ZodObject<any> | undefined> = FS extends ZodObject<any> ? <T>(overrides: {
    [K in DotPaths<FS>]?: PathValue<FS, K>;
}, fn: () => T) => T : <T>(overrides: Record<string, any>, fn: () => T) => T;
type PickFlagsFunction<FS extends ZodObject<any> | undefined> = FS extends ZodObject<any> ? {
    <K extends ReadonlyArray<DotPaths<FS> & string>>(...paths: K): K;
    <K extends ReadonlyArray<DotPaths<FS> & string>>(paths: K): K;
} : never;
interface AppScope<FS extends ZodObject<any> | undefined, SC extends ZodObject<any> | undefined> {
    flag: DotNotationFlagFunction<FS>;
    fact: FactFunction<SC>;
    overrideFlags: OverrideFlagsFunction<FS>;
    withFlags: WithFlagsFunction<FS>;
    pickFlags: PickFlagsFunction<FS>;
    getAllDefaultFlags: () => Record<string, any>;
}
/**
 * Create a new application-level evaluation scope.
 *
 * @param config.flagSchema A zod object describing the schema for flags **(required)**
 * @param config.factSchema A zod object describing the schema for facts (optional)
 *
 * @example
 * import { z } from 'zod';
 *
 * const { flag, fact, withFlags, pickFlags, overrideFlags } = createAppScope({
 *   flagSchema: z.object({
 *     ui: z.object({
 *       darkMode: z.boolean().default(false),
 *       theme:    z.object({
 *         primary: z.string().default('#00f'),
 *       }),
 *     }),
 *     api: z.object({
       endpoint: z.string().default('/api')
     }),
 *   }),
 *   factSchema: z.object({
 *     userAction: z.string(),
 *     timing: z.number(),
 *   }),
 * });
 *
 * // Typed flag access
 * const dark = flag('ui.darkMode'); // inferred boolean
 * const theme = flag('ui.theme'); // entire object
 * const primary = flag('ui.theme.primary'); // '#00f'
 * const endpoint = flag('api.endpoint'); // uses schema default
 *
 * // Typed fact recording
 * fact('userAction', 'clicked_button');
 * fact('timing', 1250);
 *
 * // Temporarily override flags for a block of code
 * withFlags({ 'ui.darkMode': true }, () => {
 *   // code here, `ui.darkMode` will be true in this block and reset after
 * });
 *
 * // Override flags globally for the current evaluation run
 * overrideFlags({ 'api.endpoint': '/custom' });
 */
declare function createAppScope<FlagSchema extends ZodObject<any>, FactSchema extends ZodObject<any> | undefined = undefined>(config: AllFieldsHaveDefaults<FlagSchema> extends true ? AppScopeConfig<FlagSchema, FactSchema> : {
    flagSchema: FlagSchema;
    factSchema?: FactSchema;
    __error__: 'createAppScope: flagSchema must have .default() for all leaf fields';
}): AppScope<FlagSchema, FactSchema>;

interface EvalContextData<Flags = any, Facts = any> {
    flags: Partial<Flags>;
    facts: Partial<Facts>;
    configScope?: ReturnType<typeof createAppScope>;
    pickedFlags?: string[];
    outOfScopeFlags?: {
        flagPath: string;
        accessedAt: number;
        stackTrace: string[];
    }[];
    parent?: EvalContextData<Flags, Facts>;
    overrides?: Record<string, any>;
    accessedFlagKeys?: string[];
}
declare function getEvalContext<Flags extends Record<string, unknown> = any, Facts extends Record<string, unknown> = any>(): EvalContextData<Flags, Facts>;
declare function withEvalContext<T>(options: {
    initialFlags?: Record<string, any>;
    pickedFlags?: string[];
} | undefined, fn: () => T): T;

interface EvalBuilder<AllowedFlags extends Record<string, any> = {}, TInput extends string | Record<string, any> = string, TExpected extends string | Record<string, any> = string, TOutput extends string | Record<string, any> = string> {
    withFlags<F extends Partial<AllowedFlags>>(flags: F): EvalBuilder<AllowedFlags, TInput, TExpected, TOutput>;
    withModel(model: string): EvalBuilder<AllowedFlags, TInput, TExpected, TOutput>;
    withTimeout(timeout: number): EvalBuilder<AllowedFlags, TInput, TExpected, TOutput>;
    run(suffix?: string): void;
}
/**
 * Create a new eval builder that can be composed with .withFlags(), .run(), etc.
 * This is the new API alongside the existing Eval() function.
 */
declare function defineEval<TInput extends string | Record<string, any> = string, TExpected extends string | Record<string, any> = string, TOutput extends string | Record<string, any> = string, AllowedFlags extends Record<string, any> = {}>(name: string, params: EvalParams<TInput, TExpected, TOutput>): EvalBuilder<AllowedFlags, TInput, TExpected, TOutput>;
/**
 * Pre-typed defineEval for app-specific flag/fact types.
 * Created by: const defineAppEval = createTypedDefineEval<AppFlags>();
 */
declare function createTypedDefineEval<AppFlags extends Record<string, any>>(): <TInput extends string | Record<string, any> = string, TExpected extends string | Record<string, any> = string, TOutput extends string | Record<string, any> = string>(name: string, params: EvalParams<TInput, TExpected, TOutput>) => EvalBuilder<AppFlags, TInput, TExpected, TOutput>;

/**
 * Validate CLI flag overrides against a schema early in eval execution.
 * Call this at the top of your eval file to fail fast on invalid flags.
 *
 * @param flagSchema - Zod schema to validate CLI flags against
 * @throws Error with helpful message if validation fails
 */
declare function validateCliFlags(flagSchema: ZodObject<any>): void;

export { type Case, type Chat, type EvalBuilder, type EvalContextData, type Evaluation, type Score, createScorer as Scorer, type Task, createAppScope, createTypedDefineEval, defineEval, AxiomReporter as experimental_AxiomReporter, Eval as experimental_Eval, type EvalParams as experimental_EvalParams, type EvalTask as experimental_EvalTask, getEvalContext, validateCliFlags, withEvalContext };
